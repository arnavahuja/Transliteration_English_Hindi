{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Transliteration.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN54gQSgJ/XsIF6F9O8hIWG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arnavahuja/Transliteration_English_Hindi/blob/master/Transliteration.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6lXI8Hq2FgfL",
        "colab_type": "text"
      },
      "source": [
        "Initial Imports "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7sjV37DcFf-1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "import re\n",
        "import random\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch import optim\n",
        "\n",
        "from torch.utils.data import Dataset\n",
        "import xml.etree.ElementTree as ET\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import HTML,clear_output"
      ],
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "00cEWH0PHH3K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device_gpu = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nezUxebDF_mU",
        "colab_type": "text"
      },
      "source": [
        "Basic Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R0MSnI2hEOpk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "802eabdb-305c-4d97-dc31-f4f0cf953190"
      },
      "source": [
        "\"\"\"Creating a dictionary for the english characters to be used later including a pad character\"\"\"\n",
        "\n",
        "eng_alphabets = 'ABCDEFGHIJKLMNOPQRSTUVWXYZ'\n",
        "pad_char = '-PAD-'\n",
        "\n",
        "eng_alpha2index = {pad_char:0}\n",
        "for index,alpha in enumerate(eng_alphabets):\n",
        "    eng_alpha2index[alpha] = index+1\n",
        "\n",
        "print(eng_alpha2index)"
      ],
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'-PAD-': 0, 'A': 1, 'B': 2, 'C': 3, 'D': 4, 'E': 5, 'F': 6, 'G': 7, 'H': 8, 'I': 9, 'J': 10, 'K': 11, 'L': 12, 'M': 13, 'N': 14, 'O': 15, 'P': 16, 'Q': 17, 'R': 18, 'S': 19, 'T': 20, 'U': 21, 'V': 22, 'W': 23, 'X': 24, 'Y': 25, 'Z': 26}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cVgbxa90EeYv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "21d73904-96d5-4237-caac-b2aef5a59b03"
      },
      "source": [
        "\"\"\"Creating a dictionary for the hindi characters to be used later including a pad character\"\"\"\n",
        "\n",
        "hindi_alphabets = [chr(alpha) for alpha in range(2304,2432)]            #The hindi alphabets are in the ascii range 2304 to 2431\n",
        "hindi_alpha_size = len(hindi_alphabets)\n",
        "\n",
        "hindi_alpha2index = {pad_char:0}\n",
        "for index,alpha in enumerate(hindi_alphabets):\n",
        "    hindi_alpha2index[alpha] = index+1\n",
        "\n",
        "print(hindi_alpha2index)"
      ],
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'-PAD-': 0, 'ऀ': 1, 'ँ': 2, 'ं': 3, 'ः': 4, 'ऄ': 5, 'अ': 6, 'आ': 7, 'इ': 8, 'ई': 9, 'उ': 10, 'ऊ': 11, 'ऋ': 12, 'ऌ': 13, 'ऍ': 14, 'ऎ': 15, 'ए': 16, 'ऐ': 17, 'ऑ': 18, 'ऒ': 19, 'ओ': 20, 'औ': 21, 'क': 22, 'ख': 23, 'ग': 24, 'घ': 25, 'ङ': 26, 'च': 27, 'छ': 28, 'ज': 29, 'झ': 30, 'ञ': 31, 'ट': 32, 'ठ': 33, 'ड': 34, 'ढ': 35, 'ण': 36, 'त': 37, 'थ': 38, 'द': 39, 'ध': 40, 'न': 41, 'ऩ': 42, 'प': 43, 'फ': 44, 'ब': 45, 'भ': 46, 'म': 47, 'य': 48, 'र': 49, 'ऱ': 50, 'ल': 51, 'ळ': 52, 'ऴ': 53, 'व': 54, 'श': 55, 'ष': 56, 'स': 57, 'ह': 58, 'ऺ': 59, 'ऻ': 60, '़': 61, 'ऽ': 62, 'ा': 63, 'ि': 64, 'ी': 65, 'ु': 66, 'ू': 67, 'ृ': 68, 'ॄ': 69, 'ॅ': 70, 'ॆ': 71, 'े': 72, 'ै': 73, 'ॉ': 74, 'ॊ': 75, 'ो': 76, 'ौ': 77, '्': 78, 'ॎ': 79, 'ॏ': 80, 'ॐ': 81, '॑': 82, '॒': 83, '॓': 84, '॔': 85, 'ॕ': 86, 'ॖ': 87, 'ॗ': 88, 'क़': 89, 'ख़': 90, 'ग़': 91, 'ज़': 92, 'ड़': 93, 'ढ़': 94, 'फ़': 95, 'य़': 96, 'ॠ': 97, 'ॡ': 98, 'ॢ': 99, 'ॣ': 100, '।': 101, '॥': 102, '०': 103, '१': 104, '२': 105, '३': 106, '४': 107, '५': 108, '६': 109, '७': 110, '८': 111, '९': 112, '॰': 113, 'ॱ': 114, 'ॲ': 115, 'ॳ': 116, 'ॴ': 117, 'ॵ': 118, 'ॶ': 119, 'ॷ': 120, 'ॸ': 121, 'ॹ': 122, 'ॺ': 123, 'ॻ': 124, 'ॼ': 125, 'ॽ': 126, 'ॾ': 127, 'ॿ': 128}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hh9UdrngEiDL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "non_eng_letters_regex = re.compile('[^a-zA-Z ]')        #Using regular expressions package to remove all non english letters\n",
        "\n",
        "#removing all english non-letters\n",
        "def cleanEnglishVocab(line):\n",
        "    line = line.replace('-',' ').replace(',',' ').upper()       #replacing comma and hyphen with space and converting all to upper-case\n",
        "    line = non_eng_letters_regex.sub('',line)\n",
        "    return line.split()                 #returning a list of the words in the line\n",
        "\n",
        "#removing all hindi non_letters\n",
        "def cleanHindiVocab(line):\n",
        "    line = line.replace('-',' ').replace(',',' ')               #replacing comma and hyphen with space\n",
        "    cleaned_line = ''\n",
        "    for char in line:\n",
        "        if char in hindi_alpha2index or char==' ':\n",
        "            cleaned_line += char\n",
        "    return cleaned_line.split()         #returning a list of the words in the line"
      ],
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_P5ApymsEuec",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"Finding the tensor representation of an english/hindi word\"\"\"\n",
        "def word_rep(word,letter2index,device='cpu'):\n",
        "    rep = torch.zeros(len(word)+1,1,len(letter2index)).to(device)       #Shape = (length of word,batch_size(to be used later),total letters in dictionary)\n",
        "    for letter_index,letter in enumerate(word):\n",
        "        pos = letter2index[letter]              #getting representation of letter\n",
        "        rep[letter_index][0][pos]=1             #adding the letter to the word tensor\n",
        "    pad_pos = letter2index[pad_char]            #adding padding at the end\n",
        "    rep[letter_index+1][0][pad_pos]=1\n",
        "    return rep\n",
        "\n",
        "\"\"\"Finding the tensor representation of ground truth\"\"\"\n",
        "def gt_rep(word,letter2index,device='cpu'):\n",
        "    gt_rep = torch.zeros([len(word)+1,1],dtype=torch.long).to(device)\n",
        "    for letter_index,letter in enumerate(word):\n",
        "        pos = letter2index[letter]\n",
        "        gt_rep[letter_index+1][0] = pos\n",
        "    gt_rep[letter_index+1][0] = letter2index[pad_char]\n",
        "    return gt_rep"
      ],
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uiwQ7qxXnNfJ",
        "colab_type": "text"
      },
      "source": [
        "Dataset Loader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t_e8KIShEp8g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TransliterationDataloader(Dataset):\n",
        "    \"\"\"Creating a dataloader for the dataset. Here the dataset is in the form of XML files\"\"\"\n",
        "    \"\"\"Read the xml file as well as clean the dataset for any abnormalities\"\"\"\n",
        "    def __init__(self,filename):\n",
        "        self.eng_words, self.hindi_words = self.read_XmlDataset(filename,cleanHindiVocab)\n",
        "        self.shuffle_indices = list(range(len(self.eng_words)))\n",
        "        random.shuffle(self.shuffle_indices)\n",
        "        self.shuffle_start_index = 0\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.eng_words)\n",
        "\n",
        "    def __getitem__(self,idx):\n",
        "        return self.eng_words[idx],self.hindi_words[idx]\n",
        "\n",
        "    def read_XmlDataset(self,filename,lang_vocab_cleaner):          #Takes vocab cleaner as argument for easy changing to a different language\n",
        "        transliterationCorpus = ET.parse(filename).getroot()        #Get the root of the xml tree \n",
        "        lang1_words = []\n",
        "        lang2_words = []\n",
        "        \n",
        "        for line in transliterationCorpus:                          #Iterating through the XML tree\n",
        "            wordlist1 = cleanEnglishVocab(line[0].text)\n",
        "            wordlist2 = lang_vocab_cleaner(line[1].text)\n",
        "\n",
        "            if len(wordlist1) != len(wordlist2):                                #removing abnormalities \n",
        "                print('Skipping:',line[0].text,'-',line[1].text)\n",
        "                continue\n",
        "            for word in wordlist1:\n",
        "                lang1_words.append(word)\n",
        "            for word in wordlist2:\n",
        "                lang2_words.append(word)\n",
        "        return lang1_words,lang2_words\n",
        "    \n",
        "    def get_random_sample(self):\n",
        "        return self.__getitem__(np.random.randint(len(self.eng_words)))         #getting a random datapoint from the dataset\n",
        "    \n",
        "    def get_batch_from_array(self,batch_size,array):                            \n",
        "        end = self.shuffle_start_index + batch_size\n",
        "        batch = []\n",
        "\n",
        "        if end>=len(self.eng_words):\n",
        "            batch = [array[i] for i in self.shuffle_indices[0:end%len(self.eng_words)]]\n",
        "            end = len(self.eng_words)\n",
        "        return batch + [array[i] for i in self.shuffle_indices[self.shuffle_start_index:end]]\n",
        "    \n",
        "    def get_batch(self,batch_size,post_process=True):                           #getting a batch from the dataset\n",
        "        eng_batch = self.get_batch_from_array(batch_size,self.eng_words)        \n",
        "        hindi_batch = self.get_batch_from_array(batch_size,self.hindi_words)\n",
        "        self.shuffle_start_index += batch_size+1\n",
        "        \n",
        "        #Reshuffle if 1 epoch is complete\n",
        "        if self.shuffle_start_index>=len(self.eng_words):\n",
        "            random.shuffle(self.shuffle_indices)\n",
        "            self.shuffle_start_index = 0\n",
        "        return eng_batch,hindi_batch\n",
        "            "
      ],
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SwUfWO2rEsS0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 672
        },
        "outputId": "fc28013c-23e6-4f9b-b297-b191d20168d2"
      },
      "source": [
        "train_data = TransliterationDataloader('NEWS2012TrainingEnHi13937.xml')         #Train Data\n",
        "test_data = TransliterationDataloader('NEWS2012RefEnHi1000.xml')                #Test Data"
      ],
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Skipping: BARHARWA JUNCTION - बरहरवा\n",
            "Skipping: STATE BNK TR - स्टेट बैंक ऑफ त्रावणकोर\n",
            "Skipping: SOUTH ARLINGTON CHURCH OF CHRIST - साउथ अर्लिंग्टन\n",
            "Skipping: KING EDWARD VII - किंग एडवर्ड\n",
            "Skipping: DIBANG VALLEY - दिबंगवैली\n",
            "Skipping: ORDER OF VASA - ऑडर ऑफ़ द वासा\n",
            "Skipping: AZAMNAGAR ROAD - आज़मनगर\n",
            "Skipping: CAPE TOWN - केपटाउन\n",
            "Skipping: NEW ZEALAND - न्यूज़ीलैंड\n",
            "Skipping: SEA OF THE HEBRIDES - सी ऑफ हरब्रिड्‍स\n",
            "Skipping: RAMCOIND - राम्को इंड\n",
            "Skipping: KELVINGROVE ART GALLERY AND MUSEUM - केल्व‍िनग्रोव आर्ट एण्ड म्युज़ियम\n",
            "Skipping: AUSTRALIAN NATIONAL UNIVERSITY - ऑस्ट्रेलियननेशनल यूनिवर्सिटी\n",
            "Skipping: JAHAN AARA - जहाँआरा\n",
            "Skipping: NAVABHARAT FERRO ALLOYS - नव भारत फ़ैरो अलॉय\n",
            "Skipping: RAMA LINGESHWARA - रामालिंगेश्वर\n",
            "Skipping: FAKHRUN NISA - फखरुन्निसा\n",
            "Skipping: REDIFF.COM INDIA LIMITED - रेडिफ़ डॉट कॉम इंडिया लिमिटेड\n",
            "Skipping: OMKARNATH THAKUR - ओंकार नाथ ठाकुर\n",
            "Skipping: OPENTV - ओपन टीवी\n",
            "Skipping: ENVOY COMMUNICATIONS GROUP - एन्वॉय कम्युनिकेशंस\n",
            "Skipping: WAR OF THE HOLY LEAGUE - वार ऑफ होली लीग\n",
            "Skipping: VAPARAISO CHURCH OF CHRIST - व्हापरासिओ\n",
            "Skipping: PARIS CHARLES DE GAULLE - पेरिस रॉसे चार्ल्स डे ग्यूले\n",
            "Skipping: PARKWAY APOSTOLIC - पार्क वे अपोस्टोलिक\n",
            "Skipping: MAUNA LOA - मौनालोआ\n",
            "Skipping: MASS MUTUAL LIFE - मास म्युच्युअल लाइफ़ इंश्योरेंस\n",
            "Skipping: STATS CHIPPAC - स्टेट्सचिपपैक\n",
            "Skipping: NEWFOUNDLAND - न्यू फाउंडलैंड\n",
            "Skipping: LONDONHEATHROW - लंदन हीथ्रो\n",
            "Skipping: RETALIX - रेटालिक्स लि.\n",
            "Skipping: SRISAILAM - श्री शैलम\n",
            "Skipping: KARA-KUM - काराकुम\n",
            "Skipping: WIND RIVER - विंडरिवर\n",
            "Skipping: NETAJI SUBHASH CHANDRA BOSE - नेताजी सुभाषचंद्र बोस\n",
            "Skipping: ROCKBROOK UNITED - रॉकब्रुक यूनाइटेड मेथोडिस्ट\n",
            "Skipping: WALTER SCOTT - वॉल्टरस्कॉट\n",
            "Skipping: COLOURPLUS FASHIONS - कलर प्लस फ़ैशन्स\n",
            "Skipping: BAL KRISHNA - बालकृष्णा\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wkRYVqCWEwMu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "70e5eab7-5bf2-4480-ddf7-389273b76e97"
      },
      "source": [
        "eng,hindi = train_data.get_random_sample()\n",
        "eng_rep = word_rep(eng,eng_alpha2index)\n",
        "print(eng,eng_rep.shape)"
      ],
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NEELMANI torch.Size([9, 1, 27])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0t7iPcgvEx_k",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        },
        "outputId": "b8f9fcd7-fdaa-4215-840c-fda5b2853805"
      },
      "source": [
        "hindi_gt = gt_rep(hindi,hindi_alpha2index)      \n",
        "print(hindi,hindi_gt)"
      ],
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "नीलमणि tensor([[ 0],\n",
            "        [41],\n",
            "        [65],\n",
            "        [51],\n",
            "        [47],\n",
            "        [36],\n",
            "        [ 0]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hOrTYemonVyz",
        "colab_type": "text"
      },
      "source": [
        "Tranliteration Model "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-yyNM_bVE-uC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"An encoder decoder model with attention for the transliteration tast using GRU\"\"\"\n",
        "\n",
        "MAX_OUTPUT_CHARS = 30\n",
        "class Transliteration_EncoderDecoder_Attention(nn.Module):\n",
        "\n",
        "    def __init__(self,input_size,hidden_size,output_size,verbose=False):\n",
        "        super(Transliteration_EncoderDecoder_Attention,self).__init__()         \n",
        "\n",
        "        self.hidden_size = hidden_size                                          #initializing the hidden size for the GRU\n",
        "        self.output_size = output_size                                          #initializing the output size for the GRU\n",
        "\n",
        "        self.encoder_rnn_cell = nn.GRU(input_size,hidden_size)                  \n",
        "        self.decoder_rnn_cell = nn.GRU(hidden_size*2,hidden_size)\n",
        "\n",
        "        self.h2o = nn.Linear(hidden_size,output_size)                           #output of the encoder \n",
        "        self.softmax = nn.LogSoftmax(dim=2)\n",
        "\n",
        "        self.U = nn.Linear(self.hidden_size,self.hidden_size)                   #defining the matrices for adding attention \n",
        "        self.W = nn.Linear(self.hidden_size,self.hidden_size)\n",
        "        self.Vattn = nn.Linear(self.hidden_size,1)\n",
        "        self.out2hidden = nn.Linear(self.output_size,self.hidden_size)\n",
        "\n",
        "        self.verbose = verbose                                                  #variable for bookkeeping\n",
        "    \n",
        "    def forward(self,input,max_output_chars=MAX_OUTPUT_CHARS,device='cpu',ground_truth=None):\n",
        "\n",
        "        #encoder\n",
        "        encoder_outputs,hidden = self.encoder_rnn_cell(input)\n",
        "        encoder_outputs = encoder_outputs.view(-1,self.hidden_size)\n",
        "\n",
        "        if self.verbose:\n",
        "            print('Encoder output-',encoder_outputs.shape)\n",
        "\n",
        "        #decoder\n",
        "        decoder_state = hidden\n",
        "        decoder_input = torch.zeros(1,1,self.output_size).to(device)\n",
        "        outputs = []\n",
        "\n",
        "        U = self.U(encoder_outputs)\n",
        "\n",
        "        if self.verbose:\n",
        "            print('Decoder state- ',decoder_state.shape)\n",
        "            print('Decoder intermediate input- ',decoder_input.shape)\n",
        "            print('U * Encoder output- ',U.shape)\n",
        "            \n",
        "        for i in range(max_output_chars):\n",
        "            \n",
        "            W = self.W(decoder_state.view(1,-1).repeat(encoder_outputs.shape[0],1))\n",
        "            V = self.Vattn(torch.tanh(W+U))\n",
        "            attn_weights = F.softmax(V.view(1,-1),dim=1)\n",
        "\n",
        "            if self.verbose:\n",
        "                print('W * DecoderState- ',W.shape)\n",
        "                print('V- ',V.shape)\n",
        "                print('Attn- ',attn_weights.shape)\n",
        "            \n",
        "            attn_applied = torch.bmm(attn_weights.unsqueeze(0),encoder_outputs.unsqueeze(0))\n",
        "\n",
        "            embedding = self.out2hidden(decoder_input)\n",
        "            decoder_input = torch.cat((embedding[0],attn_applied[0]),1).unsqueeze(0)\n",
        "\n",
        "            if self.verbose:\n",
        "                print('Attn LC- ',attn_applied.shape)\n",
        "                print('Decoder Input',decoder_input.shape)\n",
        "\n",
        "            out,decoder_state = self.decoder_rnn_cell(decoder_input,decoder_state)\n",
        "\n",
        "            if self.verbose:\n",
        "                print('Decoder intermediate input- ',out.shape)\n",
        "            \n",
        "            out = self.h2o(decoder_state)\n",
        "            out = self.softmax(out)\n",
        "            outputs.append(out.view(1,-1))\n",
        "\n",
        "            if self.verbose:\n",
        "                print('Decoder output',out.shape)\n",
        "                self.verbose=False\n",
        "\n",
        "            max_idx = torch.argmax(out,2,keepdim=True)\n",
        "            if not ground_truth is None:\n",
        "                max_idx = ground_truth[i].reshape(1,1,1)\n",
        "            one_hot = torch.FloatTensor(out.shape).to(device)\n",
        "            one_hot.zero_()\n",
        "            one_hot.scatter_(2,max_idx,1)\n",
        "\n",
        "            decoder_input = one_hot.detach()\n",
        "\n",
        "        return outputs"
      ],
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0qa4ndkuE5aA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"Applying model to an input and producing an output\"\"\"\n",
        "def infer(net,input,max_output_char=30,device='cpu'):\n",
        "    input_rep = word_rep(input,eng_alpha2index).to(device)\n",
        "    outputs = net(input_rep,max_output_char)\n",
        "    return outputs"
      ],
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_KFi0nT1nu3I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "net_attn = Transliteration_EncoderDecoder_Attention(len(eng_alpha2index),256,len(hindi_alpha2index),verbose=True)\n"
      ],
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NSBs813RoCIp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "outputId": "4c3375f1-4a10-4540-bb79-f521002882cc"
      },
      "source": [
        "out = infer(net_attn,'INDIA',30)"
      ],
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Encoder output- torch.Size([6, 256])\n",
            "Decoder state-  torch.Size([1, 1, 256])\n",
            "Decoder intermediate input-  torch.Size([1, 1, 129])\n",
            "U * Encoder output-  torch.Size([6, 256])\n",
            "W * DecoderState-  torch.Size([6, 256])\n",
            "V-  torch.Size([6, 1])\n",
            "Attn-  torch.Size([1, 6])\n",
            "Attn LC-  torch.Size([1, 1, 256])\n",
            "Decoder Input torch.Size([1, 1, 512])\n",
            "Decoder intermediate input-  torch.Size([1, 1, 256])\n",
            "Decoder output torch.Size([1, 1, 129])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PtxbR5_7o-5E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "net_attn.to(device_gpu)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5gNlaQ-foEKI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 538
        },
        "outputId": "fe7f95fc-5ff8-4f87-876f-5b892e041ce2"
      },
      "source": [
        "print(len(out))\n",
        "for i in range(len(out)):\n",
        "    print(out[i].shape,list(hindi_alpha2index.keys())[list(hindi_alpha2index.values()).index(torch.argmax(out[i]))])"
      ],
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "30\n",
            "torch.Size([1, 129]) ॎ\n",
            "torch.Size([1, 129]) ॎ\n",
            "torch.Size([1, 129]) ॎ\n",
            "torch.Size([1, 129]) ॎ\n",
            "torch.Size([1, 129]) ॎ\n",
            "torch.Size([1, 129]) ॎ\n",
            "torch.Size([1, 129]) ॎ\n",
            "torch.Size([1, 129]) ॎ\n",
            "torch.Size([1, 129]) ॎ\n",
            "torch.Size([1, 129]) ॎ\n",
            "torch.Size([1, 129]) ॎ\n",
            "torch.Size([1, 129]) ॎ\n",
            "torch.Size([1, 129]) ॎ\n",
            "torch.Size([1, 129]) ॎ\n",
            "torch.Size([1, 129]) ॎ\n",
            "torch.Size([1, 129]) ॎ\n",
            "torch.Size([1, 129]) ॎ\n",
            "torch.Size([1, 129]) ॎ\n",
            "torch.Size([1, 129]) ॎ\n",
            "torch.Size([1, 129]) ॎ\n",
            "torch.Size([1, 129]) ॎ\n",
            "torch.Size([1, 129]) ॎ\n",
            "torch.Size([1, 129]) ॎ\n",
            "torch.Size([1, 129]) ॎ\n",
            "torch.Size([1, 129]) ॎ\n",
            "torch.Size([1, 129]) ॎ\n",
            "torch.Size([1, 129]) ॎ\n",
            "torch.Size([1, 129]) ॎ\n",
            "torch.Size([1, 129]) ॎ\n",
            "torch.Size([1, 129]) ॎ\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KqAxNmI4niUr",
        "colab_type": "text"
      },
      "source": [
        "Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8qUFRiHgFGY-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_batch(net,opt,criterion,batch_size,device='cpu',teacher_force=False):\n",
        "    \"\"\"Training the model for a batch of data for transliteration \"\"\"\n",
        "    net.train().to(device)\n",
        "    opt.zero_grad()\n",
        "    eng_batch,hindi_batch = train_data.get_batch(batch_size)\n",
        "\n",
        "    total_loss=0\n",
        "    for i in range(batch_size):\n",
        "        input = word_rep(eng_batch[i],eng_alpha2index,device)                   #get the tensor representations of inputs\n",
        "        gt = gt_rep(hindi_batch[i],hindi_alpha2index,device)                    #get the tensor representations of ground truth\n",
        "        outputs = net(input,gt.shape[0],device,ground_truth=gt if teacher_force else None)\n",
        "\n",
        "        for index,output in enumerate(outputs):\n",
        "            loss = criterion(output,gt[index])/batch_size                       #find the loss using  negative logloss funciton\n",
        "            loss.backward(retain_graph=True)                                    \n",
        "            total_loss+=loss\n",
        "\n",
        "    opt.step()\n",
        "    return total_loss/batch_size"
      ],
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TCLc57CjFZ-W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_setup(net,lr=0.01,n_batches=100,batch_size=10,momentum=0.9,display_freq=5,device='cpu'):\n",
        "    \"\"\"Function encompassing the entire train setup\"\"\"\n",
        "    net = net.to(device)\n",
        "    criterion = nn.NLLLoss(ignore_index=-1)\n",
        "    opt = optim.Adam(net.parameters(),lr=lr,weight_decay=1e-5)       \n",
        "    teacher_force_upto = n_batches//100\n",
        "\n",
        "    loss_arr = np.zeros(n_batches+1)\n",
        "\n",
        "    for i in range(n_batches):\n",
        "        loss_arr[i+1] = (loss_arr[i]*i +  train_batch(net,opt,criterion,batch_size,device=device,teacher_force=i<teacher_force_upto))/(i+1)\n",
        "\n",
        "        if i%display_freq==0:\n",
        "            clear_output(wait=True)\n",
        "\n",
        "            print('Iteration',i,'Loss',loss_arr[i])\n",
        "            plt.figure()\n",
        "            plt.plot(loss_arr[1:i],'-*')\n",
        "            plt.xlabel('Iteration')\n",
        "            plt.ylabel('Loss')\n",
        "            plt.show()\n",
        "            print('\\n\\n')\n",
        "    torch.save(net,'model.pt')\n",
        "    return loss_arr"
      ],
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z_xj89TmFOvy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_setup(net_attn,lr=0.0001,n_batches=50,batch_size=64,display_freq=10,device=device_gpu)"
      ],
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kzqFNFl6nljx",
        "colab_type": "text"
      },
      "source": [
        "Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ERFvLyeFQcs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def calc_accuracy(net,device='cpu'):\n",
        "    \"\"\"Function for calculating the test accuracy\"\"\"\n",
        "    net = net.eval().to(device)\n",
        "    predictions = []\n",
        "    accuracy = 0\n",
        "    for i in range(len(test_data)):\n",
        "        eng,hindi = test_data[i]\n",
        "        gt = gt_rep(hindi,hindi_alpha2index,device)\n",
        "        outputs = infer(net,eng,gt.shape[0],device)\n",
        "        correct = 0\n",
        "        for index,out in enumerate(outputs):\n",
        "            val,indices = out.topk(1)\n",
        "            hindi_pos = indices.tolist()[0]\n",
        "            if hindi_pos[0] == gt[index][0]:\n",
        "                correct+=1\n",
        "        accuracy+=correct/gt.shape[0]\n",
        "    accuracy/=len(test_data)\n",
        "    return accuracy"
      ],
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gE1l2DAQFS6_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def calc_accuracy_train(net,device='cpu'):\n",
        "    \"\"\"Funciton for calculating train accuracy\"\"\"\n",
        "    net = net.eval().to(device)\n",
        "    predictions = []\n",
        "    accuracy = 0\n",
        "    for i in range(len(train_data)):                #iterating over the train data \n",
        "        eng,hindi = train_data[i]\n",
        "        gt = gt_rep(hindi,hindi_alpha2index,device)\n",
        "        outputs = infer(net,eng,gt.shape[0],device)\n",
        "        correct = 0\n",
        "        for index,out in enumerate(outputs):        \n",
        "            val,indices = out.topk(1)               #calculating accuracy using topk accuracy function. (k can be used as an input parameter)\n",
        "            hindi_pos = indices.tolist()[0]\n",
        "            if hindi_pos[0] == gt[index][0]:\n",
        "                correct+=1\n",
        "        accuracy+=correct/gt.shape[0]\n",
        "    accuracy/=len(train_data)\n",
        "    return accuracy"
      ],
      "execution_count": 119,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U0um8stYFVAK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "accuracy = calc_accuracy(net_attn)*100\n",
        "print(accuracy)"
      ],
      "execution_count": 120,
      "outputs": []
    }
  ]
}